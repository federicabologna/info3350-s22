{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400fe12e-0c3f-4ae1-9bab-7051b5903739",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 26: Working with Social Media Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8b47d-b40a-4243-8bcc-252de1f54fd0",
   "metadata": {},
   "source": [
    "## What is social media data?\n",
    "\n",
    " * Collected from social networks\n",
    " * Different kinds:\n",
    "     * Example: posts, comments, likes, followers, clicks, shares (reposts and retweets), comments.\n",
    "     * Numerical or textual format\n",
    "     * We'll focus on textual data\n",
    "\n",
    "![](images/socialmediadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74934b-2f64-4fbf-9113-15102abac712",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Why is working with social media data important?\n",
    " \n",
    " * Literature can tell us about the past\n",
    "     * Look backward in time\n",
    "     * We don't have a lot of digitized textual data from the past (letters? birth certificates?)\n",
    " * Social media data can tell us about the present moment\n",
    "     * Look at the present or forward in time\n",
    "     * We have lots of data from social networks!\n",
    " * It can tell us about\n",
    "     * human behaviour\n",
    "     * language\n",
    "     * current events\n",
    "\n",
    "![](images/interestingpapers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea00c80-be11-4f61-a8e4-3298e0ae3c7c",
   "metadata": {},
   "source": [
    "## Differences between literary texts and texts from social media\n",
    " \n",
    " * Literary texts\n",
    "     * Historical\n",
    "     * Long format\n",
    "     * Formally edited and published\n",
    " * Text from social media data\n",
    "     * Contemporary (more or less)\n",
    "     * Shorter format\n",
    "     * Not formally edited and published"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66056b9-9bd3-4491-af84-309c45c2c529",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Do students at Cornell talk about student life differently in 2020 vs 2022?\n",
    "\n",
    "For the scope of this exercise, we will only focus on Reddit posts and comments published in March and April of 2020, and in March and April of 2022.\n",
    "To investigate this question we will:\n",
    "- Scrape posts and comments;\n",
    "- Gather information about the corpus of post and comments;\n",
    "- Deduplicate and clean the corpus;\n",
    "- Perform topic modeling;\n",
    "- Evaluate topic modeling;\n",
    "- Perform classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf82a7-139c-428c-a2c9-17c2a265913e",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c8eef-d90b-4822-a2a1-209426476825",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up scraping\n",
    "\n",
    "Install some new packages for this lecture. We have to use `pip`, since none of these are available via `conda`.\n",
    "\n",
    "**Note that `tomotopy` does not work natively on Apple Silicon Macs.** If you're running python via Rosetta, you'll be fine. If you're running M1-native python, you're out of luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036349d1-2c97-4f16-978c-0ab195700a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psaw in /Users/mwilkens/opt/miniforge3/envs/3350/lib/python3.10/site-packages (0.1.0)\n",
      "Collecting little_mallet_wrapper\n",
      "  Using cached little_mallet_wrapper-0.5.0-py3-none-any.whl (19 kB)\n",
      "Collecting Levenshtein\n",
      "  Using cached Levenshtein-0.18.1-cp310-cp310-macosx_11_0_arm64.whl (230 kB)\n",
      "Requirement already satisfied: Click in /Users/mwilkens/opt/miniforge3/envs/3350/lib/python3.10/site-packages (from psaw) (8.0.4)\n",
      "Requirement already satisfied: requests in /Users/mwilkens/opt/miniforge3/envs/3350/lib/python3.10/site-packages (from psaw) (2.27.1)\n",
      "Collecting rapidfuzz<3.0.0,>=2.0.1\n",
      "  Using cached rapidfuzz-2.0.11-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Collecting jarowinkler<1.1.0,>=1.0.2\n",
      "  Using cached jarowinkler-1.0.2-cp310-cp310-macosx_11_0_arm64.whl (56 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mwilkens/opt/miniforge3/envs/3350/lib/python3.10/site-packages (from requests->psaw) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mwilkens/opt/miniforge3/envs/3350/lib/python3.10/site-packages (from requests->psaw) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mwilkens/opt/miniforge3/envs/3350/lib/python3.10/site-packages (from requests->psaw) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mwilkens/opt/miniforge3/envs/3350/lib/python3.10/site-packages (from requests->psaw) (2.0.12)\n",
      "Installing collected packages: little_mallet_wrapper, jarowinkler, rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.18.1 jarowinkler-1.0.2 little_mallet_wrapper-0.5.0 rapidfuzz-2.0.11\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install psaw little_mallet_wrapper Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb97e5-baae-4291-aeb4-0e1235475063",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install tomotopy # does not work on M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53963f8a-260e-4636-b558-37959a6ffbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "base_path = os.path.join('reddit_data')  # creating a directory for the data\n",
    "if not os.path.exists(base_path):  # if it does not exist\n",
    "    os.makedirs(base_path)         # create it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9df937-4b29-4eff-a1de-75f7377518ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scraping functions\n",
    "\n",
    "Here are the two functions for scraping posts and comments respectively from the subreddit of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68bc9710-d62e-4c12-b879-032c49139269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Maria Antoniak's code with minor modifications \"\"\"\n",
    "def scrape_posts_from_subreddit(subreddit, api, year, month, end_date):\n",
    "    '''\n",
    "    Takes the name of a subreddit, the PushshiftApi, a year and month to scrape from\n",
    "    '''\n",
    "    start_epoch = int(datetime(year, month, 1).timestamp())  # convert date into unicode timestamp\n",
    "    end_epoch = int(datetime(year, month, end_date).timestamp())\n",
    "\n",
    "    gen = api.search_submissions(after=start_epoch,\n",
    "                                 before=end_epoch,\n",
    "                                 subreddit=subreddit,\n",
    "                                 filter=['url', 'author', 'created_utc',  # info we want about the post\n",
    "                                         'title', 'subreddit', 'selftext',\n",
    "                                         'num_comments', 'score', 'link_flair_text', 'id'])\n",
    "\n",
    "    max_response_cache = 100000\n",
    "    scraped_posts = []\n",
    "    for _post in gen:\n",
    "        scraped_posts.append(_post)\n",
    "        if len(scraped_posts) >= max_response_cache:  # avoid requesting more posts than allowed\n",
    "            break\n",
    "\n",
    "    scraped_posts_df = pd.DataFrame([p.d_ for p in scraped_posts])\n",
    "\n",
    "    return scraped_posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63161418-5d26-405b-be31-2a7b122c622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Maria Antoniak's code with minor modifications \"\"\"\n",
    "def scrape_comments_from_subreddit(subreddit, api, year, month, end_date):\n",
    "    '''\n",
    "    Takes the name of a subreddit, the PushshiftApi, a year and month to scrape from\n",
    "    '''\n",
    "    start_epoch = int(datetime(year, month, 1).timestamp())  # convert date into unicode timestamp\n",
    "    end_epoch = int(datetime(year, month, end_date).timestamp())\n",
    "\n",
    "    gen = api.search_comments(after=start_epoch,\n",
    "                              before=end_epoch,\n",
    "                              subreddit=subreddit,\n",
    "                              filter=['author', 'body', 'created_utc', # info we want about the comment\n",
    "                                      'id', 'link_id', 'parent_id',\n",
    "                                      'reply_delay', 'score', 'subreddit'])\n",
    "\n",
    "    max_response_cache = 100000\n",
    "    scraped_comments = []\n",
    "    for _comment in gen:\n",
    "        scraped_comments.append(_comment)\n",
    "        if len(scraped_comments) >= max_response_cache:  # avoid requesting more posts than allowed\n",
    "            break\n",
    "    scraped_comments_df = pd.DataFrame([p.d_ for p in scraped_comments])\n",
    "\n",
    "    return scraped_comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af1fd6-f78a-48c3-bb61-20ce6ddc35b2",
   "metadata": {},
   "source": [
    "### Scrape!\n",
    "\n",
    "Here we will decide:\n",
    "- which subreddit to scrape,\n",
    "- which content type to scrape from that subreddit,\n",
    "- and which dates we want to scrape.\n",
    "And we will set off the previous scraping functions accordingly.\n",
    "\n",
    "We will save files to **pickle format**, why?\n",
    "- To avoid confusion when reading and writing them! Texts contain commas, and it is possible that pandas might read them as separators when reading CSV files.\n",
    "\n",
    "NOTE ON DIRECTORIES:\n",
    "- Our jupyter notebook is in a folder on our machine\n",
    "  - inside that folder we previously we created a folder `reddit_data`\n",
    "    - inside `reddit_data` we will create a folder named after the subreddit we will scrape `Cornell`\n",
    "      - inside `Cornell` we will create one folder for each of the two content types `posts` and `comments`\n",
    "        - inside `posts` we will store all the data about the posts of the Cornell subreddit\n",
    "        - inside `comments` we will store all the data about the comments of the Cornell subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c084d28c-39c4-47f1-bffc-555436826232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Maria Antoniak's code with minor modifications \"\"\"\n",
    "def scrape_subreddit(_target_subreddits, _target_types, _years):\n",
    "    '''\n",
    "    Takes a list of subreddits, a list of types of content to scrape, and a list of years to scrape from\n",
    "    '''\n",
    "    \n",
    "    api = PushshiftAPI()\n",
    "\n",
    "    print('Number of PushshiftApi shards that are not working:', api.metadata_.get('shards'))  # check if any Pushshift shards are down!\n",
    "    \n",
    "    for _subreddit in _target_subreddits:\n",
    "        for _target_type in _target_types:\n",
    "            for _year in _years:\n",
    "                if _year < 2022:\n",
    "                    months = [3, 4]\n",
    "                    end_dates = [31, 30]\n",
    "                elif _year == 2022:\n",
    "                    months = [3, 4]  # months to scrape\n",
    "                    end_dates = [31, 30]  # last day of the month\n",
    "\n",
    "                for _month, _end_date in zip(months, end_dates):\n",
    "                    _output_directory_path = os.path.join(base_path, _subreddit, _target_type)  # directory to store scraped data\n",
    "                                                                                                # by subreddit and type of content\n",
    "                    if not os.path.exists(_output_directory_path):  # if it does not exist\n",
    "                        os.makedirs(_output_directory_path)         # create it!\n",
    "\n",
    "                    _file_name = _subreddit + '-' + str(_year) + '-' + str(_month) + '.pkl'  # filename of the csv with scraped data\n",
    "\n",
    "                    # scrape only if output file does not already exist\n",
    "                    if _file_name not in os.listdir(_output_directory_path):\n",
    "\n",
    "                        print(str(datetime.now()) + ' ' + ': Scraping r/' + _subreddit + ' ' + str(_year) + '-' + str(_month) + '...')\n",
    "\n",
    "                        if _target_type == 'posts':\n",
    "                            _posts_df = scrape_posts_from_subreddit(_subreddit, api, _year, _month, _end_date)\n",
    "                            if not _posts_df.empty:\n",
    "                                _posts_df.to_pickle(os.path.join(_output_directory_path, _file_name), protocol=4)\n",
    "\n",
    "                        if _target_type == 'comments':\n",
    "                            _comments_df = scrape_comments_from_subreddit(_subreddit, api, _year, _month, _end_date)\n",
    "                            if not _comments_df.empty:\n",
    "                                _comments_df.to_pickle(os.path.join(_output_directory_path, _file_name), protocol=4)\n",
    "\n",
    "    print(str(datetime.now()) + ' ' + ': Done scraping!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f70c83-c566-4783-9c16-a2a84addd1de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PushshiftApi shards that are not working: None\n",
      "2022-05-02 21:03:03.221418 : Done scraping!\n"
     ]
    }
   ],
   "source": [
    "target_subreddits = ['cornell']  # subreddits to scrape\n",
    "target_types = ['posts', 'comments']  # type of content to scrape\n",
    "years = [2020, 2022]  # years to scrape\n",
    "scrape_subreddit(target_subreddits, target_types, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60915b6e-7c31-4a22-af62-a477de7f96ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Combine posts and comments for one subreddit\n",
    "\n",
    "Here we will combine the pickle files with all the posts from the subreddit and the pickle files with all the comments from the same subreddit into one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c4b9bd5-7f2c-4859-a4af-071667218989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_one_subreddit(_subreddit):  # creating csv with all of a subreddit's posts and comments\n",
    "\n",
    "    df_d = {'author': [], 'id': [], 'type': [], 'text': [],   # create a dictionary\n",
    "            'url': [], 'link_id': [], 'parent_id': [],\n",
    "            'subreddit': [], 'created_utc': []}\n",
    "    \n",
    "    subreddit_pkl_path = os.path.join('reddit_data', _subreddit, f'{_subreddit}.pkl') # file with all the data\n",
    "    if not os.path.exists(subreddit_pkl_path):  # if file does not exist\n",
    "        \n",
    "        for target_type in ['posts', 'comments']:\n",
    "            files_directory_path = os.path.join('reddit_data', _subreddit, target_type)  # directory where scraped data is depending on subreddit and type of content\n",
    "            all_target_type_files = glob.glob(os.path.join(files_directory_path, \"*.pkl\"))  # select all appropriate pickle files\n",
    "            for f in all_target_type_files:  # we read each pickle file and include the info we want in the dictionary\n",
    "                df = pd.read_pickle(f)\n",
    "\n",
    "\n",
    "                if target_type == 'posts':\n",
    "                    for index, row in df.iterrows():\n",
    "                        df_d['author'].append(row['author'])\n",
    "                        df_d['id'].append(f\"{row['subreddit']}_{row['id']}_post\")  # id of the post, 'Endo_xyz123_post'\n",
    "                        df_d['type'].append('post')\n",
    "                        df_d['text'].append(row['selftext'])  # textual content of the post\n",
    "                        df_d['url'].append(row['url'])  # url of the post\n",
    "                        df_d['link_id'].append('N/A')\n",
    "                        df_d['parent_id'].append('N/A')\n",
    "                        df_d['subreddit'].append(row['subreddit'])\n",
    "                        df_d['created_utc'].append(row['created_utc'])  # utc time stamp of the post\n",
    "\n",
    "\n",
    "                elif target_type == 'comments':\n",
    "                    for index, row in df.iterrows():\n",
    "                        df_d['author'].append(row['author'])\n",
    "                        df_d['id'].append(f\"{row['subreddit']}_{row['id']}_comment\")\n",
    "                        df_d['type'].append('comment')\n",
    "                        df_d['text'].append(row['body'])  # textual content of the comment\n",
    "                        df_d['url'].append(f\"http://www.reddit.com/r/Endo/comments/{row['link_id'].split('_')[1]}/\")  # url of the post\n",
    "                        df_d['link_id'].append(row['link_id'])\n",
    "                        df_d['parent_id'].append(row['parent_id'])\n",
    "                        df_d['subreddit'].append(row['subreddit'])\n",
    "                        df_d['created_utc'].append(row['created_utc'])  # utc time stamp of the post\n",
    "\n",
    "\n",
    "        subreddit_df = pd.DataFrame.from_dict(df_d)  # create pandas dataframe from dictionary\n",
    "        subreddit_df.sort_values('created_utc', inplace=True, ignore_index=True)  # order dataframe by date of post\n",
    "        subreddit_df['time'] = pd.to_datetime(subreddit_df['created_utc'], unit='s').apply(lambda x: x.to_datetime64())  # convert timestamp to date\n",
    "        subreddit_df['date'] = subreddit_df['time'].apply(lambda x: str(x).split(' ')[0])\n",
    "        subreddit_df['year'] = subreddit_df['time'].apply(lambda x: str(x).split('-')[0])\n",
    "        subreddit_df.drop(columns=['time'])\n",
    "        \n",
    "        subreddit_df.to_pickle(subreddit_pkl_path, protocol=4)  # saving it to pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3311db3f-bb0c-4f9d-9cf3-0f93f30caba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for subreddit in target_subreddits:\n",
    "    combine_one_subreddit(subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c5f81-6ee9-4576-b71d-6c937843a81c",
   "metadata": {},
   "source": [
    "## Some info on the corpus\n",
    "\n",
    "Before performing any analysis it's important to get to know our texts. Characteristics about our social media texts affect how we will carry out our analysis. Let's check:\n",
    "- how long the texts are,\n",
    "- how many words are in the vocabulary of the corpus,\n",
    "- what the most commons words are in the corpus etc.\n",
    "\n",
    "This information will inform how we will clean the texts and perform topic modeling on them in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b627e5-9622-46c8-8639-30e2ee054da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0baac51b-74f2-4fd8-b2ca-677f8889c4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60023\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(os.path.join('reddit_data', 'cornell', 'cornell.pkl'))\n",
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c6e7fe9-3870-4df0-8fe0-6061ee757f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(df, _type):\n",
    "    if _type != 'corpus':\n",
    "        vectorizer = CountVectorizer(        # Token counts with stopwords\n",
    "            input = 'content',               # input is a string of texts\n",
    "            encoding = 'utf-8',\n",
    "            strip_accents = 'unicode',\n",
    "            lowercase = True\n",
    "        )\n",
    "\n",
    "        texts = df['text'].astype('string').tolist()\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        print(f\"Total vectorized words in the corpus of {_type}:\", X.sum())\n",
    "        print(f\"Average vectorized {_type} length:\", int(X.sum()/X.shape[0]), \"tokens\")\n",
    "    \n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            input = 'content',\n",
    "            encoding = 'utf-8',\n",
    "            strip_accents = 'unicode',\n",
    "            lowercase = True,\n",
    "            stop_words = 'english'          # remove stopwords\n",
    "        )\n",
    "        \n",
    "        texts = df['text'].astype('string').tolist()\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        sum_words = X.sum(axis=0)\n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "        words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        print('Top words in the combined corpus of posts and comments after removing stopwords:')\n",
    "        for word, freq in words_freq[:30]:\n",
    "            print(word, '\\t', freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ee9650-0e5a-4552-983f-82f113cfcd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts in r/Cornell: 7310\n",
      "Number of comments in r/Cornell: 52713\n",
      "Number of posts and comments from 2020 in r/Cornell: 22160\n",
      "Number of posts and comments from 2022 in r/Cornell: 37863\n",
      "Total vectorized words in the corpus of posts: 283923\n",
      "Average vectorized posts length: 38 tokens\n",
      "Total vectorized words in the corpus of comments: 1362887\n",
      "Average vectorized comments length: 25 tokens\n"
     ]
    }
   ],
   "source": [
    "df_posts = df.loc[df['type'] == 'post'].copy()\n",
    "df_comments = df.loc[df['type'] == 'comment'].copy()\n",
    "df_2020 = df.loc[df['year'] == '2020'].copy()\n",
    "df_2022 = df.loc[df['year'] == '2022'].copy()\n",
    "print(f'Number of posts in r/Cornell:', len(df_posts))\n",
    "print(f'Number of comments in r/Cornell:', len(df_comments))\n",
    "print(f'Number of posts and comments from 2020 in r/Cornell:', len(df_2020))\n",
    "print(f'Number of posts and comments from 2022 in r/Cornell:', len(df_2022))\n",
    "print_info(df_posts, 'posts')\n",
    "print_info(df_comments, 'comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66fe64db-c4d7-470d-a2a0-163be9b22b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in the combined corpus of posts and comments after removing stopwords:\n",
      "just \t 8224\n",
      "cornell \t 8148\n",
      "like \t 7191\n",
      "people \t 6822\n",
      "don \t 6465\n",
      "think \t 4714\n",
      "know \t 4409\n",
      "time \t 4061\n",
      "really \t 3925\n",
      "class \t 3718\n",
      "students \t 3680\n",
      "good \t 3184\n",
      "classes \t 3178\n",
      "want \t 3172\n",
      "ve \t 3113\n",
      "https \t 2970\n",
      "school \t 2892\n",
      "make \t 2570\n",
      "year \t 2502\n",
      "going \t 2469\n",
      "semester \t 2463\n",
      "ll \t 2433\n",
      "work \t 2428\n",
      "need \t 2356\n",
      "housing \t 2335\n",
      "campus \t 2318\n",
      "got \t 2310\n",
      "lot \t 2291\n",
      "way \t 2121\n",
      "sure \t 2047\n"
     ]
    }
   ],
   "source": [
    "print_info(df, 'corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe1dda-0584-43c2-91b9-03f2677b3eb6",
   "metadata": {},
   "source": [
    "## Pre-process the corpus\n",
    "\n",
    "\n",
    "When scraping Reddit or other platforms, it is important to consider how the platform is used by users, to have an idea of the kind of texts we might find.\n",
    "\n",
    "A few things to keep in mind:\n",
    "- the content on these platforms is **barely curated**. Moderators and bots designed for content moderation often just remove the most offensive and inflammatory content.\n",
    "  - Unless you are dealing with a special subreddit/community that enforces very strict norms, you will find funky looking, uninformative, and bot-generated texts.\n",
    "- In most social platforms, social interaction can revolve around **images**. Unless alt-text is provided (sadly, basically never), we cannot access that information using our NLP tools.\n",
    "  - Therefore some texts will look funky for that reason. Such documents are generally short.\n",
    "- On Reddit, content shows up depending on the up- and down-votes it receives. If a user's post gets ignored by their community, they sometime repost it to receive an answer.\n",
    "  - Thus, in your corpus, you might find 5, 10, 20 **duplicates** of an individual post.\n",
    "\n",
    "HOWEVER, how much and whether you need to clean your corpus highly depends on **a few factors**:\n",
    "- The goal of your analysis, your question\n",
    "- The community you are analyzing\n",
    "  - **Be respectful!** This content might look weird to you, but can mean a lot to the members of the community\n",
    "  - Keep in mind that you are analyzing someone's behavior and interaction online. Put yourself in their shoes :)\n",
    "- The techniques you are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c45cac83-ff70-4407-ba15-fbd964417050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import little_mallet_wrapper as lmw\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5aced7-f176-4ab8-8b72-103c3af1f370",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deduplicating function\n",
    "\n",
    "This is far from an optimal function for getting rid of duplicates. For sake of time, we will make sure that content posted by the same user is not duplicated, and that the previous post - chrnologically - is not identical.\n",
    "\n",
    "We will use the Levenshtein distance. It measures how different two strings are. It is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one string into the other. It is useful because it does not require tokenization. So we can get rid of most of the duplicates before cleaning the data, saving us some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "814e69aa-eb57-4d7e-a064-8afae63042bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(_df):  # function to find duplicated posts in the data\n",
    "\n",
    "    prev_doc = ''\n",
    "    map_dict = {}  # dict of authors' posts\n",
    "    duplicate_indexes = []  # list of duplicates' indexes for removal from dataframe\n",
    "    for index, row in _df.iterrows():  # iterate over posts\n",
    "        author = row['author']\n",
    "        doc = row['text']\n",
    "\n",
    "        # if author info is available we compare each post with previous ones by the same author\n",
    "        # we compare/calculate the similarity between the posts using the Levenshtein distance\n",
    "        if author != '[deleted]':\n",
    "            if author in map_dict.keys():\n",
    "                flag = 0\n",
    "                idx = 0\n",
    "                while idx < len(map_dict[author]) and flag == 0:\n",
    "                    lev = Levenshtein.ratio(doc, map_dict[author][idx])\n",
    "                    if lev > 0.99:\n",
    "                        duplicate_indexes.append(index)\n",
    "                        flag = 1\n",
    "                    idx += 1\n",
    "                if flag == 0:\n",
    "                    map_dict[author].append(doc)\n",
    "            else:\n",
    "                map_dict[author] = [doc]\n",
    "\n",
    "        # if author info is not available we compare each post with the preceding one chronologically\n",
    "        else:\n",
    "            lev = Levenshtein.ratio(row['text'], prev_doc)\n",
    "            if lev > 0.90:\n",
    "                duplicate_indexes.append(index)\n",
    "\n",
    "        prev_doc = doc\n",
    "\n",
    "    return duplicate_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42b574a4-68b6-4362-bcf8-46af69cdf14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 1453\n"
     ]
    }
   ],
   "source": [
    "dupes = find_duplicates(df)  # find duplicates\n",
    "df.drop(dupes, inplace=True)  # removing duplicates\n",
    "print(f'Number of duplicates: {len(dupes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d1d11-a710-448e-bb8c-49d1c240f883",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleaning function\n",
    "\n",
    "Before we perform topic modeling it's important we remove messages generated by bots or that are not diverse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "604002d5-7fe8-4655-8e56-18bd02360538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_docs(raw_df, _subreddit):\n",
    "    '''\n",
    "    Takes the full corpus, a file path. It cleans all the documents (removes punctuation and stopwords). It saves the clean corpus in a json file\n",
    "    '''\n",
    "    clean_docs_file = os.path.join('reddit_data', _subreddit, f'clean_{_subreddit}.pkl')\n",
    "    if not os.path.exists(clean_docs_file): \n",
    "        \n",
    "        clean_d = {'id':[], 'clean':[], 'og':[], 'year':[], 'date':[]}\n",
    "\n",
    "        for index, row in raw_df.iterrows():                               # iterating over posts and comments\n",
    "            if 'bot' not in row['author'] and 'Bot' not in row['author']:  # if author is not a bot\n",
    "                clean_doc_st = lmw.process_string(row['text'])             # cleaning documents\n",
    "                clean_doc_l = [t for t in clean_doc_st.split(' ')]\n",
    "                if len(set(clean_doc_l))>5 and 'bot' not in clean_doc_l:  # exclude posts that have less than 5 different words\n",
    "                                                                          # or that contain word 'bot'\n",
    "                    clean_d['clean'].append(clean_doc_l)\n",
    "                    clean_d['id'].append(row['id'])\n",
    "                    clean_d['og'].append(row['text'])\n",
    "                    clean_d['year'].append(row['year'])\n",
    "                    clean_d['date'].append(row['date'])\n",
    "\n",
    "        with open(clean_docs_file, 'w') as jsonfile:  # creating a file with the dict of documents to topic model\n",
    "            json.dump(clean_d, jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fca7154-40d4-4bd6-b20c-f5c1297c5232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 189 µs, sys: 101 µs, total: 290 µs\n",
      "Wall time: 266 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for subreddit in target_subreddits:\n",
    "        cleaning_docs(df, subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3567cd5-5f10-4d09-9ed0-42b178f9a498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topic modeling\n",
    "\n",
    "What topics appear in Cornell's subreddit?\n",
    "\n",
    "To perform LDA, we will be using `tomotopy` a new, fast and easy-to-use package for topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4681ba0-2bf4-4304-a574-434e18b681ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fd834-6d78-400f-944c-50f30106c933",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Topic modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb5f97-7e9e-439c-bc4f-726cb906fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mixture of Matthew Wilkens' and Melanie Walsh's code\"\"\"\n",
    "def perform_topic_modeling(_doc_ids, _clean_docs, _num_topics, _rm_top, _topwords_file):\n",
    "    '''\n",
    "    Takes a list of document ids, a list of clean docs to perform LDA on, a number of topics, a number of top words to remove,\n",
    "    a file path for the top words file. It performs topic modeling on the documents, then creates the top words file and a doc-term matrix.\n",
    "    '''\n",
    "                                          # setting and loading the LDA model\n",
    "    lda_model = tp.LDAModel(k=_num_topics,      # number of topics in the model\n",
    "                            min_df=3,           # remove words that occur in less than n documents\n",
    "                            rm_top=_rm_top)     # remove n most frequent words\n",
    "    for doc in _clean_docs:\n",
    "        lda_model.add_doc(doc)  # adding document to the model\n",
    "\n",
    "    iterations = 10\n",
    "    for i in range(0, 100, iterations):  # train model 10 times with 10 iterations at each training = 100 iterations\n",
    "        lda_model.train(iterations)\n",
    "        print(f'Iteration: {i}\\tLog-likelihood: {lda_model.ll_per_word}')\n",
    "\n",
    "    # Writing the document with the TOP WORDS per TOPIC\n",
    "    num_top_words = 25                                      # number of top words to print for each topic\n",
    "    with open(_topwords_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"\\nTopics in LDA model: {_num_topics} topics {_rm_top} removed top words\\n\\n\")\n",
    "                                                            # write settings of the model in file\n",
    "        topic_individual_words = []\n",
    "        \n",
    "        for topic_number in range(0, _num_topics):                  # for each topic number in the total number of topics\n",
    "            topic_words = ' '.join(                                 # string of top words in the topic\n",
    "                word for word, prob in lda_model.get_topic_words(topic_id=topic_number, top_n=num_top_words))\n",
    "                                                # get_topic_words is a tomotopy function that returns a dict of words and their probabilities\n",
    "            \n",
    "            topic_individual_words.append(topic_words.split(' '))   # append list of the topic's top words for later\n",
    "            file.write(f\"Topic {topic_number}\\n{topic_words}\\n\\n\")  # write topic number and top words in file\n",
    "\n",
    "            \n",
    "    # TOPIC DISTRIBUTIONS\n",
    "    topic_distributions = [list(doc.get_topic_dist()) for doc in lda_model.docs]  # list of lists of topic distributions for each document\n",
    "    topic_results = []\n",
    "    for topic_distribution in topic_distributions:\n",
    "        topic_results.append({'topic_distribution': topic_distribution}) # adding dicts of topic distributions to list\n",
    "    \n",
    "    df = pd.DataFrame(topic_results, index=_doc_ids) \n",
    "                                                    # df where each row is the list of topic distributions of a document, s_ids are the ids of the sentences\n",
    "    column_names = [f\"Topic {number} {topic[0]}\" for number, topic in enumerate(topic_individual_words)]  # create list of column names from topic numbers and top words\n",
    "    \n",
    "    df[column_names] = pd.DataFrame(df['topic_distribution'].tolist(), index=df.index)\n",
    "                                    # df where topic distributions are not in a list and match the list of column names\n",
    "    df = df.drop('topic_distribution', axis='columns')  # drop old topic distributions' column\n",
    "    \n",
    "    dominant_topic = np.argmax(df.values, axis=1)       # get dominant topic for each document\n",
    "    df['dominant_topic'] = dominant_topic\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbfa4f-b0ed-4672-acb3-d17b19a0b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topic_modeling(_subreddit):\n",
    "    tomo_folder = os.path.join('output', 'topic_modeling')  # results' folder\n",
    "    if not os.path.exists(tomo_folder):  # create folder if it doesn't exist\n",
    "        os.makedirs(tomo_folder)\n",
    "    \n",
    "    clean_docs_file = os.path.join('reddit_data', _subreddit, f'clean_{_subreddit}.pkl')\n",
    "    with open(clean_docs_file) as json_file:\n",
    "        clean_docs_dict = json.load(json_file)\n",
    "    doc_ids = clean_docs_dict['id']       # list of ids of clean documents\n",
    "    clean_docs = clean_docs_dict['clean']  # list of clean documents to perform topic modeling on                       \n",
    "\n",
    "    \n",
    "    print(\"Performing Topic Modeling...\")     # for loop to run multiple models with different settings with one execution\n",
    "    for num_topics in [10, 20]:            # for number of topics\n",
    "        for rm_top in [5]:                 # for number of most frequent words to remove\n",
    "\n",
    "            topwords_file = os.path.join(tomo_folder, f'{subreddit}-{num_topics}_{rm_top}.txt')  # path for top words file\n",
    "            docterm_file = os.path.join(tomo_folder,f'{subreddit}-{num_topics}_{rm_top}.pkl')  # path for doc-topic matrix file\n",
    "            if not os.path.exists(topwords_file) or not os.path.exists(docterm_file):         # if result files don't exist, performs topic model\n",
    "                \n",
    "                start = datetime.now()\n",
    "                lda_dtm = perform_topic_modeling(doc_ids, clean_docs, num_topics, rm_top, topwords_file)\n",
    "                lda_dtm['og_doc'] = clean_docs_dict['og']    # list of original documents for evaluation\n",
    "                lda_dtm['year'] = clean_docs_dict['year']\n",
    "                lda_dtm['date'] = clean_docs_dict['date']\n",
    "                lda_dtm.to_pickle(docterm_file, protocol=4)  # convert doc-topic df in csv file\n",
    "                print(f'{str(datetime.now())}____Topic modeling {num_topics}, {rm_top} time:____{str(datetime.now() - start)}\\n')  # print timing of topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f604d-bce4-4780-ac58-6a85a4ec3985",
   "metadata": {},
   "source": [
    "### Run Topic Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd7365-eca3-45d6-82a4-bab31ca692eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for subreddit in target_subreddits:\n",
    "    run_topic_modeling('cornell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568c7fd-5f5d-4164-9832-23dbd297de93",
   "metadata": {},
   "source": [
    "### Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb54479-820a-4693-94e2-52b4b3745690",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "def print_top_docs_per_topic(_df, _txtfile):\n",
    "    \n",
    "    with open(_txtfile, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        idx = 3\n",
    "        while idx < len(lines):\n",
    "            topic_line = lines[idx]\n",
    "            words_line = lines[idx+1]\n",
    "            n = topic_line.split()[1]\n",
    "            word_1 = words_line.split()[0]\n",
    "            print(f'{topic_line}{words_line}')\n",
    "            for doc in _df.sort_values(f'Topic {n} {word_1}', ascending=False).og_doc.tolist()[5:10]:\n",
    "                print(doc)\n",
    "                print(\"_________\")\n",
    "            print('\\n\\n')\n",
    "            idx += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6318aeb-85d0-44e1-8ac2-6edacd657033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_removed = 5\n",
    "n_topics = 20\n",
    "tomo_folder = os.path.join('output', 'topic_modeling')\n",
    "tomo_pklfile = os.path.join(tomo_folder, f'cornell-{n_topics}_{n_removed}.pkl')\n",
    "tomo_txtfile = os.path.join(tomo_folder, f'cornell-{n_topics}_{n_removed}.txt')\n",
    "tomo_df = pd.read_pickle(tomo_pklfile)\n",
    "print(f'Number of documents in topic model: {len(tomo_df)}')\n",
    "print_top_docs_per_topic(tomo_df, tomo_txtfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a03b5-1125-40bc-86bb-d772512c41cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification using topic distributions\n",
    "\n",
    "Let's check if we can predict whether a post/comment is from 2020 or 2022 using its topic distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac73799-cb41-4d80-b5c9-4ca0406fb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955f878-bc4d-44b0-994d-801deed74a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the performance of our simple classifiers\n",
    "# Freebie function to summarize and display classifier scores\n",
    "def compare_scores(scores_dict):\n",
    "    '''\n",
    "    Takes a dictionary of cross_validate scores.\n",
    "    Returns a color-coded Pandas dataframe that summarizes those scores.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(scores_dict).T.applymap(np.mean).style.background_gradient(cmap='RdYlGn')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29505b4d-332f-4999-90e5-f6b3c6279aa5",
   "metadata": {},
   "source": [
    "### 20 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af5853-0de4-488f-b44b-a74e8a1738a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomo_shuffled = tomo_df.sample(frac=1)\n",
    "\n",
    "tomo_shuffled['y_year'] = tomo_shuffled['year'].apply(lambda x: 0 if x == '2020' else 1)\n",
    "y_year = tomo_shuffled['y_year'].tolist()\n",
    "x_docterm = tomo_shuffled[tomo_df.columns[:n_topics].tolist()]\n",
    "X_docterm = StandardScaler().fit_transform(x_docterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08cb94-2db2-4fa7-a27e-a67d4d6e665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Logit':LogisticRegression(),\n",
    "    'Random forest':RandomForestClassifier(),\n",
    "    'SVM':SVC()\n",
    "}\n",
    "\n",
    "scores1 = {} # Store cross-validation results in a dictionary\n",
    "for classifier in classifiers: \n",
    "    scores1[classifier] = cross_validate( # perform cross-validation\n",
    "        classifiers[classifier], # classifier object\n",
    "        X_docterm, # feature matrix\n",
    "        y_year, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy','precision', 'recall', 'f1'] # scoring methods\n",
    "    )\n",
    "    \n",
    "compare_scores(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8bd69-056b-49b4-8adb-ccd2712d2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = f_classif #f is much faster than mutal_info, but not as robust\n",
    "selector = SelectKBest(method, k=5)\n",
    "X_best = selector.fit_transform(X_docterm, y_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57beb41-cedd-4a9a-8414-ba3f6d0c9da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores2 = {} # Store cross-validation results in a dictionary\n",
    "for classifier in classifiers: \n",
    "    scores2[classifier] = cross_validate( # perform cross-validation\n",
    "        classifiers[classifier], # classifier object\n",
    "        X_best, # feature matrix\n",
    "        y_year, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy','precision', 'recall', 'f1'] # scoring methods\n",
    "    )\n",
    "\n",
    "compare_scores(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51552f-35cd-4da5-a9d0-d38165c60d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = tomo_df.columns[:n_topics].tolist()\n",
    "top_features = sorted(zip(all_features, selector.scores_), key=lambda x: x[1], reverse=True)\n",
    "for top_feature in top_features[:5]:\n",
    "    print(f'{top_feature[0]} \\t\\tscore: {top_feature[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f5a66-8b3a-4e03-b22e-55955fe3b711",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Permutation test\n",
    "\n",
    "In order to find out whether differences between topic distributions are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1bf8e9-1b33-4214-a943-c6a63b3518dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1f95f-7e3e-486a-a170-7ed669eb0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute(input_array):\n",
    "    # shuffle is inplace, so copy to preserve input\n",
    "    permuted = input_array.copy().values  # convert to numpy array, avoiding warning\n",
    "    np.random.shuffle(permuted)\n",
    "    return pd.Series(permuted)  # convert back to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a527f3-3246-4339-9aa3-ec1008c31f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(ddf, raw_column):\n",
    "    \n",
    "    # Difference between the mean of the values in the first half and the mean of the values in the second half of the corpus\n",
    "    column = f'{raw_column}_z'\n",
    "    ddf[column] = stats.zscore(ddf[raw_column])\n",
    "    real_mean_before = ddf.loc[ddf['year'] == '2020'][column].mean()\n",
    "    real_mean_after = ddf.loc[ddf['year'] == '2022'][column].mean()\n",
    "    diff_real = real_mean_before - real_mean_after \n",
    "    \n",
    "    # Performing 1,000 permutations\n",
    "    n_permutations = 1000\n",
    "    flag = 0\n",
    "    for i in range(n_permutations):\n",
    "        copy = ddf.copy()  # we copy the original dataframe with the observed data\n",
    "        copy['year'] = permute(copy['year'])  # we shuffle the 'year' column\n",
    "        mean_before = copy.loc[copy['year'] == '2020'][column].mean()\n",
    "        mean_after = copy.loc[copy['year'] == '2022'][column].mean()\n",
    "        diff_perm = mean_before - mean_after  # we calculate the difference between the means of the two halves of the corpus\n",
    "        if diff_real > 0:  # if real difference is a positive number\n",
    "            if diff_real > diff_perm:  # we test if the observed difference is greater\n",
    "                flag += 1\n",
    "        if diff_real < 0:  # if real difference is a positive number\n",
    "            if diff_real < diff_perm:  # we test if the observed difference is lesser\n",
    "                flag += 1  # we keep count of the number of times the observed difference is larger\n",
    "    p = (n_permutations-flag)/n_permutations\n",
    "    \n",
    "    return diff_real, flag, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588906b5-331f-4d30-b1ef-fb1635db7cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Permutation test on the difference between the daily relative occurence of the symptoms label\n",
    "# in the first and second halves of the corpus\n",
    "for _column in tomo_df.columns[:n_topics]:\n",
    "    diff, flag_value, p_value = permutation_test(tomo_df, _column)\n",
    "    print(f'{_column} in 2020 vs 2022')\n",
    "    print(f'Observed difference: {diff}')\n",
    "    print(f'Number of times observed difference is larger than permutated: {flag_value}')\n",
    "    print(f'P-value: {p_value}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
